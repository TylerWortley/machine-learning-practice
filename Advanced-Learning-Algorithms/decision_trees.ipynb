{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learning\n",
    "- Decision 1: How to choose what feature to split on at each node to maximize purity (minimize impurity)\n",
    "- Decision 2: When do you stop splitting?\n",
    "    - When a node is 100% one class\n",
    "    - WHen splitting a node will result in exceeding a maximum depth\n",
    "    - When improvements in purity score are below a threshold\n",
    "    - When the number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "- measure of impirity of data\n",
    "- Entropy function: H(p1 )\n",
    "- closer to 50/50 mix, closer to maximum entropy (1.0)\n",
    "\n",
    "**H(p1) = -p1*log2(p1) - p0*log2(p0)** = -p1*log2(p1) - (1-p1)*log2(-p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Split: Information Gain\n",
    "- node with lots of examples and high entropy is worse than node with few examples and high entropy\n",
    "- **Information Gain:** the reduction in entropy that you get from your tree resulting from making a split\n",
    "    - p1_left = fraction in left split with a positive label\n",
    "    - w_left = fraction of examples out of all root examples that went to left node\n",
    "\n",
    "    - p1_right = fraction in right split with a positive label\n",
    "    - w_right = fraction of examples out of all root examples that went to right node\n",
    "\n",
    "    - p1_root = fraction of positive labels in root node\n",
    "\n",
    "***Information Gain = H(p1_root) - [w_left * H(p1_left) + w_right * H(p1_right)]***\n",
    "\n",
    "- for regression trees, p1_root, p1_left and p1_right are **varainces** -> split on biggest variance reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load some examples. X_train contains 3 features: ear shape (1 = points), face shape (1 = round), and whiskers (1 = present)\n",
    "# y_train contains labels, 1 = cat, 0 = otherwise\n",
    "X_train = np.array([[1, 1, 1],\n",
    "[0, 0, 1],\n",
    " [0, 1, 0],\n",
    " [1, 0, 1],\n",
    " [1, 1, 1],\n",
    " [1, 1, 0],\n",
    " [0, 0, 0],\n",
    " [1, 1, 0],\n",
    " [0, 1, 0],\n",
    " [0, 1, 0]])\n",
    "\n",
    "y_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy function\n",
    "def entropy(p):\n",
    "    # handels boundary conditions where function would be mathematically undefined\n",
    "    if p == 1 or p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(X, index_feature):\n",
    "    left_indicies = []\n",
    "    right_indicies = []\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        if x[index_feature] == 1:\n",
    "            left_indicies.append(i)\n",
    "        else:\n",
    "            right_indicies.append(i)\n",
    "    \n",
    "    return left_indicies, right_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(X, y, left_indicies, right_indicies):\n",
    "    \n",
    "    # proportion of examples at node\n",
    "    w_left = len(left_indicies) / len(X)\n",
    "    w_right = len(right_indicies) / len(X)\n",
    "\n",
    "    # proportion of labels in node\n",
    "    p_left = sum(y[left_indicies]) / len(left_indicies)\n",
    "    p_right = sum(y[right_indicies]) / len(right_indicies)\n",
    "\n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, left_indicies, right_indicies):\n",
    "    p_node = sum(y) / len(y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy(X, y, left_indicies, right_indicies)\n",
    "\n",
    "    info_gain = h_node - w_entropy\n",
    "    \n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: Ear Shape, information gain if we split the root node using this feature: 0.28\n",
      "Feature: Face Shape, information gain if we split the root node using this feature: 0.03\n",
      "Feature: Whiskers, information gain if we split the root node using this feature: 0.12\n"
     ]
    }
   ],
   "source": [
    "# information gain for splitting root for each feature\n",
    "\n",
    "for i, feature_name in enumerate(['Ear Shape', 'Face Shape', 'Whiskers']):\n",
    "    left_indices, right_indices = split_indices(X_train, i)\n",
    "    i_gain = information_gain(X_train, y_train, left_indices, right_indices)\n",
    "    print(f\"Feature: {feature_name}, information gain if we split the root node using this feature: {i_gain:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Ensembles ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- majority vote amonst the trees gives overall result\n",
    "- create new random training set using sampling with replacement (can include repeats)\n",
    "\n",
    "Procedure - Bagged Trees\n",
    "given training set of size m\n",
    "for b = 1 to B:\n",
    "    use sampling with replacement to create a new training set of size m\n",
    "    train a decision tree on the new data set\n",
    "\n",
    "- **Reccommended 64-128 trees**. Larger B never hurts performance but diminishing returns\n",
    "\n",
    "\n",
    "Random Forest Algorithm\n",
    "- At each node, when choosing a feature to use to split, if n features are available, pick a random subset of k < features and allow the algorithm to only choose from that subset of features\n",
    "    - typically k = âˆšn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (eXtreme Gradient) Boost\n",
    "- modification to bagged decision tree algorithm\n",
    "\n",
    "given training set of size m\n",
    "for b = 1 to B:\n",
    "    use sampling with replacement to create a new training set of size m, **but instead of picking from all examples with equal (1/m) probability, make it more likely to pick misclassified examples from previously trained trees\n",
    "    train a decision tree on the new data set\n",
    "\n",
    "\n",
    "\n",
    "**Benefits of XG boost:**\n",
    "- open source implementation of boosted trees\n",
    "- fast efficient implementation\n",
    "- good choice of default splitting criteria and criteria for when to stop splitting\n",
    "- built in regularization to prevent overfitting\n",
    "- highly competitive algorithm for machine learning competitions (i.e Kaggle competitions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees vs Neural Networks\n",
    "**Decision Trees and Tree Ensembles**\n",
    "- works well on tabular (structured) data\n",
    "- not recommended for unstructured data (images, audio, text, overall things not stored as tables)\n",
    "- fast!\n",
    "- small decision trees may be interpreted by humans \n",
    "\n",
    "**Neural Netwrorks**\n",
    "- works well on all types of data, including tabular (structured) and unstructured data\n",
    "- may be slower than a decision tree\n",
    "- works with transfer learning\n",
    "- when building a system of multiple models working together, it might be easier to string together multiple neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>HeartDisease</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>ChestPainType_ASY</th>\n",
       "      <th>...</th>\n",
       "      <th>ChestPainType_NAP</th>\n",
       "      <th>ChestPainType_TA</th>\n",
       "      <th>RestingECG_LVH</th>\n",
       "      <th>RestingECG_Normal</th>\n",
       "      <th>RestingECG_ST</th>\n",
       "      <th>ExerciseAngina_N</th>\n",
       "      <th>ExerciseAngina_Y</th>\n",
       "      <th>ST_Slope_Down</th>\n",
       "      <th>ST_Slope_Flat</th>\n",
       "      <th>ST_Slope_Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  RestingBP  Cholesterol  FastingBS  MaxHR  Oldpeak  HeartDisease  \\\n",
       "0   40        140          289          0    172      0.0             0   \n",
       "1   49        160          180          0    156      1.0             1   \n",
       "2   37        130          283          0     98      0.0             0   \n",
       "3   48        138          214          0    108      1.5             1   \n",
       "4   54        150          195          0    122      0.0             0   \n",
       "\n",
       "   Sex_F  Sex_M  ChestPainType_ASY  ...  ChestPainType_NAP  ChestPainType_TA  \\\n",
       "0  False   True              False  ...              False             False   \n",
       "1   True  False              False  ...               True             False   \n",
       "2  False   True              False  ...              False             False   \n",
       "3   True  False               True  ...              False             False   \n",
       "4  False   True              False  ...               True             False   \n",
       "\n",
       "   RestingECG_LVH  RestingECG_Normal  RestingECG_ST  ExerciseAngina_N  \\\n",
       "0           False               True          False              True   \n",
       "1           False               True          False              True   \n",
       "2           False              False           True              True   \n",
       "3           False               True          False             False   \n",
       "4           False               True          False              True   \n",
       "\n",
       "   ExerciseAngina_Y  ST_Slope_Down  ST_Slope_Flat  ST_Slope_Up  \n",
       "0             False          False          False         True  \n",
       "1             False          False           True        False  \n",
       "2             False          False          False         True  \n",
       "3              True          False           True        False  \n",
       "4             False          False          False         True  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 55\n",
    "\n",
    "# kaggle dataset\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "cat_variables = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "# use get_dummies to one-hot encode the variables with three or more values\n",
    "df = pd.get_dummies(data = df, prefix = cat_variables, columns = cat_variables)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 734\n",
      "validation samples: 184\n",
      "target proportion: 0.5518\n"
     ]
    }
   ],
   "source": [
    "# target is HeartDisease, all other features are potential predictors\n",
    "features = [x for x in df.columns if x not in 'HeartDisease']\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[features], df['HeartDisease'], train_size = 0.8, random_state = RANDOM_STATE)\n",
    "\n",
    "print(f'train samples: {len(X_train)}')\n",
    "print(f'validation samples: {len(X_val)}')\n",
    "print(f'target proportion: {sum(y_train)/len(y_train):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "min_samples_split_list = [2, 10, 30, 50, 100, 200, 300, 700]\n",
    "max_depth_list = [1, 2, 3, 4, 8, 16, 32, 64, None]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
