{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learning\n",
    "- Decision 1: How to choose what feature to split on at each node to maximize purity (minimize impurity)\n",
    "- Decision 2: When do you stop splitting?\n",
    "    - When a node is 100% one class\n",
    "    - WHen splitting a node will result in exceeding a maximum depth\n",
    "    - When improvements in purity score are below a threshold\n",
    "    - When the number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "- measure of impirity of data\n",
    "- Entropy function: H(p1 )\n",
    "- closer to 50/50 mix, closer to maximum entropy (1.0)\n",
    "\n",
    "**H(p1) = -p1*log2(p1) - p0*log2(p0)** = -p1*log2(p1) - (1-p1)*log2(-p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Split: Information Gain\n",
    "- node with lots of examples and high entropy is worse than node with few examples and high entropy\n",
    "- **Information Gain:** the reduction in entropy that you get from your tree resulting from making a split\n",
    "    - p1_left = fraction in left split with a positive label\n",
    "    - w_left = fraction of examples out of all root examples that went to left node\n",
    "\n",
    "    - p1_right = fraction in right split with a positive label\n",
    "    - w_right = fraction of examples out of all root examples that went to right node\n",
    "\n",
    "    - p1_root = fraction of positive labels in root node\n",
    "\n",
    "***Information Gain = H(p1_root) - [w_left * H(p1_left) * w_right * H(p1_right)]***"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
