{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learning\n",
    "- Decision 1: How to choose what feature to split on at each node to maximize purity (minimize impurity)\n",
    "- Decision 2: When do you stop splitting?\n",
    "    - When a node is 100% one class\n",
    "    - WHen splitting a node will result in exceeding a maximum depth\n",
    "    - When improvements in purity score are below a threshold\n",
    "    - When the number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "- measure of impirity of data\n",
    "- Entropy function: H(p1 )\n",
    "- closer to 50/50 mix, closer to maximum entropy (1.0)\n",
    "\n",
    "**H(p1) = -p1*log2(p1) - p0*log2(p0)** = -p1*log2(p1) - (1-p1)*log2(-p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Split: Information Gain\n",
    "- node with lots of examples and high entropy is worse than node with few examples and high entropy\n",
    "- **Information Gain:** the reduction in entropy that you get from your tree resulting from making a split\n",
    "    - p1_left = fraction in left split with a positive label\n",
    "    - w_left = fraction of examples out of all root examples that went to left node\n",
    "\n",
    "    - p1_right = fraction in right split with a positive label\n",
    "    - w_right = fraction of examples out of all root examples that went to right node\n",
    "\n",
    "    - p1_root = fraction of positive labels in root node\n",
    "\n",
    "***Information Gain = H(p1_root) - [w_left * H(p1_left) + w_right * H(p1_right)]***\n",
    "\n",
    "- for regression trees, p1_root, p1_left and p1_right are **varainces** -> split on biggest variance reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load some examples. X_train contains 3 features: ear shape (1 = points), face shape (1 = round), and whiskers (1 = present)\n",
    "# y_train contains labels, 1 = cat, 0 = otherwise\n",
    "X_train = np.array([[1, 1, 1],\n",
    "[0, 0, 1],\n",
    " [0, 1, 0],\n",
    " [1, 0, 1],\n",
    " [1, 1, 1],\n",
    " [1, 1, 0],\n",
    " [0, 0, 0],\n",
    " [1, 1, 0],\n",
    " [0, 1, 0],\n",
    " [0, 1, 0]])\n",
    "\n",
    "y_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy function\n",
    "def entropy(p):\n",
    "    # handels boundary conditions where function would be mathematically undefined\n",
    "    if p == 1 or p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(X, index_feature):\n",
    "    left_indicies = []\n",
    "    right_indicies = []\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "        if x[index_feature] == 1:\n",
    "            left_indicies.append(i)\n",
    "        else:\n",
    "            right_indicies.append(i)\n",
    "    \n",
    "    return left_indicies, right_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(X, y, left_indicies, right_indicies):\n",
    "    \n",
    "    # proportion of examples at node\n",
    "    w_left = len(left_indicies) / len(X)\n",
    "    w_right = len(right_indicies) / len(X)\n",
    "\n",
    "    # proportion of labels in node\n",
    "    p_left = sum(y[left_indicies]) / len(left_indicies)\n",
    "    p_right = sum(y[right_indicies]) / len(right_indicies)\n",
    "\n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, left_indicies, right_indicies):\n",
    "    p_node = sum(y) / len(y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy(X, y, left_indicies, right_indicies)\n",
    "\n",
    "    info_gain = h_node - w_entropy\n",
    "    \n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: Ear Shape, information gain if we split the root node using this feature: 0.28\n",
      "Feature: Face Shape, information gain if we split the root node using this feature: 0.03\n",
      "Feature: Whiskers, information gain if we split the root node using this feature: 0.12\n"
     ]
    }
   ],
   "source": [
    "# information gain for splitting root for each feature\n",
    "\n",
    "for i, feature_name in enumerate(['Ear Shape', 'Face Shape', 'Whiskers']):\n",
    "    left_indices, right_indices = split_indices(X_train, i)\n",
    "    i_gain = information_gain(X_train, y_train, left_indices, right_indices)\n",
    "    print(f\"Feature: {feature_name}, information gain if we split the root node using this feature: {i_gain:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Ensembles ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- majority vote amonst the trees gives overall result\n",
    "- create new random training set using sampling with replacement (can include repeats)\n",
    "\n",
    "Procedure - Bagged Trees\n",
    "given training set of size m\n",
    "for b = 1 to B:\n",
    "    use sampling with replacement to create a new training set of size m\n",
    "    train a decision tree on the new data set\n",
    "\n",
    "- **Reccommended 64-128 trees**. Larger B never hurts performance but diminishing returns\n",
    "\n",
    "\n",
    "Random Forest Algorithm\n",
    "- At each node, when choosing a feature to use to split, if n features are available, pick a random subset of k < features and allow the algorithm to only choose from that subset of features\n",
    "    - typically k = âˆšn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (eXtreme Gradient) Boost\n",
    "- modification to bagged decision tree algorithm\n",
    "\n",
    "given training set of size m\n",
    "for b = 1 to B:\n",
    "    use sampling with replacement to create a new training set of size m, **but instead of picking from all examples with equal (1/m) probability, make it more likely to pick misclassified examples from previously trained trees\n",
    "    train a decision tree on the new data set\n",
    "\n",
    "\n",
    "\n",
    "**Benefits of XG boost:**\n",
    "- open source implementation of boosted trees\n",
    "- fast efficient implementation\n",
    "- good choice of default splitting criteria and criteria for when to stop splitting\n",
    "- built in regularization to prevent overfitting\n",
    "- highly competitive algorithm for machine learning competitions (i.e Kaggle competitions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
