{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the goal is to find a function that maps the **state (s)** to an **action (a)**\n",
    "- uses a reward function to train model **R(s)**\n",
    "\n",
    "Applications\n",
    "- Controlling robots\n",
    "- factory optimization\n",
    "- financial (stock) trading\n",
    "- playing games (including video games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Return in Reinforcement Learning\n",
    "-  **discount factor (γ)**: modifies reward credited to each step, discounting rewards further in the future (often a number close to 1)\n",
    "- the **return** in reinforcement learning is the **sum of the rewards the system gets** but **weighted by the discount factor** -> rewards in the future are weighted by the discount factor raised to a higher power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Decisions: Policies in Reinforcement Learning\n",
    "- a policy function, **π(s) = a**, tells you what **action (a)** to take in a given **state (s)**\n",
    "\n",
    "- **goal**: fund a policy that tells you what action to take in every state (s) so as to maximize the return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "- model for sequential decision making when outcomes are uncertain and partly controllable\n",
    "- \"Markov\" means that the future only depends on the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-Action Value Function (Q Function, Q*. Optimal Q Function)\n",
    "- a function typicall denoted by **Q(s, a)**\n",
    "    - gives a number equal to the return if you start in a **state (s)**, take the **action (a)** once, and behave optimally after that\n",
    "    - tells us how good it is to take action a in state s\n",
    "    - the best possible return from **state (s)** is **max Q(s, a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "- helps compute the state-action value function (**Q(s, a)**)\n",
    "- terms\n",
    "    - **s: current state**\n",
    "    - **a: current action**\n",
    "    - **s': state you get to after taking action a**\n",
    "    - **a': action that you take in state s'**\n",
    "    - **R(s) = rewards of current state**\n",
    "- Equation: **Q(s, a) = R(s) + γ(max Q(s', a'))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Environments\n",
    "- sometimes, when you take an action, the outcome is not always completely reliable (i.e slippery floor causes robot to move in wrong direction, device thrown off balance, wind, etc)\n",
    "- there isn't one sequence of rewards that you are guarenteed to see\n",
    "- now, we are trying to maximize the **average or expected return**\n",
    "- **Q(s, a) = R(s) + γ* E(max Q(s', a'))**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
