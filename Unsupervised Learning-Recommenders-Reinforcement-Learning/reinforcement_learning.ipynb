{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the goal is to find a function that maps the **state (s)** to an **action (a)**\n",
    "- uses a reward function to train model **R(s)**\n",
    "\n",
    "Applications\n",
    "- Controlling robots\n",
    "- factory optimization\n",
    "- financial (stock) trading\n",
    "- playing games (including video games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Return in Reinforcement Learning\n",
    "-  **discount factor (γ)**: modifies reward credited to each step, discounting rewards further in the future (often a number close to 1)\n",
    "- the **return** in reinforcement learning is the **sum of the rewards the system gets** but **weighted by the discount factor** -> rewards in the future are weighted by the discount factor raised to a higher power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Decisions: Policies in Reinforcement Learning\n",
    "- a policy function, **π(s) = a**, tells you what **action (a)** to take in a given **state (s)**\n",
    "\n",
    "- **goal**: fund a policy that tells you what action to take in every state (s) so as to maximize the return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "- model for sequential decision making when outcomes are uncertain and partly controllable\n",
    "- \"Markov\" means that the future only depends on the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-Action Value Function (Q Function, Q*. Optimal Q Function)\n",
    "- a function typicall denoted by **Q(s, a)**\n",
    "    - gives a number equal to the return if you start in a **state (s)**, take the **action (a)** once, and behave optimally after that\n",
    "    - tells us how good it is to take action a in state s\n",
    "    - the best possible return from **state (s)** is **max Q(s, a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "- helps compute the state-action value function (**Q(s, a)**)\n",
    "- terms\n",
    "    - **s: current state**\n",
    "    - **a: current action**\n",
    "    - **s': state you get to after taking action a**\n",
    "    - **a': action that you take in state s'**\n",
    "    - **R(s) = rewards of current state**\n",
    "- Equation: **Q(s, a) = R(s) + γ(max Q(s', a'))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Environments\n",
    "- sometimes, when you take an action, the outcome is not always completely reliable (i.e slippery floor causes robot to move in wrong direction, device thrown off balance, wind, etc)\n",
    "- there isn't one sequence of rewards that you are guarenteed to see\n",
    "- now, we are trying to maximize the **average or expected return**\n",
    "- **Q(s, a) = R(s) + γ* E(max Q(s', a'))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the State-Value Function\n",
    "- key idea: train a neural network to compute / approximate the state action value function ((Q(s, a))) that will in turn let us pick good actions\n",
    "- get Q(s, a) from NN, compute all options and select action that yeilds best reward\n",
    "- for training \n",
    "    $$ x^{(1)} = (s^{(1)}, a^{(1)}) $$ \n",
    "\n",
    "    $$ y^{(1)} = R(s^{(1)}) + γmax Q(s^{'(1)}, a^{'}) $$ \n",
    "- at every step, Q will be a guess (that will get better over time)\n",
    "\n",
    "### Full Algorithm\n",
    "1. Initialize neural network parameters randomly as an initial random guess for the Q function \n",
    "2. Repeat \n",
    "    - Take actions in lunar lander. Get (s, a, R(s), s')\n",
    "    - Store 10,000 most recent tuples (s, a, R(s), s') (called the replay buffer)\n",
    "    - Occasionally train NN by creating training set of 10,000 examples -> x =(s, a), y = R(s) + γ*max(Q(s',a'))\n",
    "    - Train such that $Q_{new} (s, a) ~= y$\n",
    "    - set $ Q = Q_{new}$ \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Refinement: Improved Neural Network Architecture\n",
    "- previous would have to carry out inference in the neural network separetly for as many times as there are actions, to pick the action that gives the largest Q value\n",
    "- instead, it is more efficient to train a single neural network to **output all action values simultaneously**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Refinement: ϵ-greedy policy\n",
    "- this is used to pick the actions while still learning (first bullet in full algorithm)\n",
    "- with probability 0.95 (**95%** of the time), pick the action a that **maximizes Q(s, a)** -> Greedy/Explotation Step\n",
    "- with probaility 0.05 (other **5%** of the time), pick action a **randomly** -> Exploration Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Refinement: Mini-Batch and Soft Updates\n",
    "- **Mini Batches**\n",
    "    - every step of gradient descent requires computing the error across every single example which can be quite slow\n",
    "    - instead, pick a subset (m') and instead of using all examples, use m' examples to speed up the algorithm\n",
    "    - good for both supervised and reinforcement learning\n",
    "- **Soft Updates**\n",
    "    - whenever we train a new neural network (Q_new), we will only update with a little portion of the new value \n",
    "        - $ W = 0.01*W_{new} + 0.99*W $\n",
    "        - $ B = 0.01*B_{new} + 0.99*B $\n",
    "    - increases convergence reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning - Lunar Lander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `numpy` is a package for scientific computing in python.\n",
    "- `deque` will be our data structure for our memory buffer.\n",
    "- `namedtuple` will be used to store the experience tuples.\n",
    "- The `gym` toolkit is a collection of environments that can be used to test reinforcement learning algorithms. We should note that in this notebook we are using `gym` version `0.24.0`.\n",
    "- `PIL.Image` and `pyvirtualdisplay` are needed to render the Lunar Lander environment.\n",
    "- We will use several modules from the `tensorflow.keras` framework for building deep learning models.\n",
    "- `utils` is a module that contains helper functions for this assignment. You do not need to modify the code in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Action Space\n",
    "\n",
    "The agent has four discrete actions available:\n",
    "\n",
    "* Do nothing.\n",
    "* Fire right engine.\n",
    "* Fire main engine.\n",
    "* Fire left engine.\n",
    "\n",
    "Each action has a corresponding numerical value:\n",
    "\n",
    "```python\n",
    "Do nothing = 0\n",
    "Fire right engine = 1\n",
    "Fire main engine = 2\n",
    "Fire left engine = 3\n",
    "```\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Observation Space\n",
    "\n",
    "The agent's observation space consists of a state vector with 8 variables:\n",
    "\n",
    "* Its $(x,y)$ coordinates. The landing pad is always at coordinates $(0,0)$.\n",
    "* Its linear velocities $(\\dot x,\\dot y)$.\n",
    "* Its angle $\\theta$.\n",
    "* Its angular velocity $\\dot \\theta$.\n",
    "* Two booleans, $l$ and $r$, that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Rewards\n",
    "\n",
    "After every step, a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "- is increased/decreased the closer/further the lander is to the landing pad.\n",
    "- is increased/decreased the slower/faster the lander is moving.\n",
    "- is decreased the more the lander is tilted (angle not horizontal).\n",
    "- is increased by 10 points for each leg that is in contact with the ground.\n",
    "- is decreased by 0.03 points each frame a side engine is firing.\n",
    "- is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receives an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Episode Termination\n",
    "\n",
    "An episode ends (i.e the environment enters a terminal state) if:\n",
    "\n",
    "* The lunar lander crashes (i.e if the body of the lunar lander comes in contact with the surface of the moon).\n",
    "\n",
    "* The absolute value of the lander's $x$-coordinate is greater than 1 (i.e. it goes beyond the left or right border)\n",
    "\n",
    "You can check out the [Open AI Gym documentation](https://www.gymlibrary.dev/environments/box2d/lunar_lander/) for a full description of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v3', render_mode  ='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .reset() method to reset the environemtn to initial state. lander starts at the top center of the environment and we can render the first frame \n",
    "# of the environmnet with .render() method\n",
    "env.reset()\n",
    "Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of state vector and number of valid actions required to build neural netowrk later\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Interacting with the Gym Environment\n",
    "\n",
    "In the standard “agent-environment loop” formalism, an agent interacts with the environment in discrete time steps $t=0,1,2,...$. At each time step $t$, the agent uses a policy $\\pi$ to select an action $A_t$ based on its observation of the environment's state $S_t$. The agent receives a numerical reward $R_t$ and on the next time step, moves to a new state $S_{t+1}$.\n",
    "\n",
    "<a name=\"5.1\"></a>\n",
    "### 5.1 Exploring the Environment's Dynamics\n",
    "\n",
    "In Open AI's Gym environments, we use the `.step()` method to run a single time step of the environment's dynamics. In the version of `gym` that we are using the `.step()` method accepts an action and returns four values:\n",
    "\n",
    "* `observation` (**object**): an environment-specific object representing your observation of the environment. In the Lunar Lander environment this corresponds to a numpy array containing the positions and velocities of the lander as described in section [3.2 Observation Space](#3.2).\n",
    "\n",
    "\n",
    "* `reward` (**float**): amount of reward returned as a result of taking the given action. In the Lunar Lander environment this corresponds to a float of type `numpy.float64` as described in section [3.3 Rewards](#3.3).\n",
    "\n",
    "\n",
    "* `done` (**boolean**): When done is `True`, it indicates the episode has terminated and it’s time to reset the environment. \n",
    "\n",
    "\n",
    "* `info` (**dictionary**): diagnostic information useful for debugging. We won't be using this variable in this notebook but it is shown here for completeness.\n",
    "\n",
    "To begin an episode, we need to reset the environment to an initial state. We do this by using the `.reset()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the environment is reset, the agent can start taking actions in the environment by using the `.step()` method. Note that the agent can only take one action per time step. \n",
    "\n",
    "In the cell below you can select different actions and see how the returned values change depending on the action taken. Remember that in this environment the agent has four discrete actions available and we specify them in code by using their corresponding numerical value:\n",
    "\n",
    "```python\n",
    "Do nothing = 0\n",
    "Fire right engine = 1\n",
    "Fire main engine = 2\n",
    "Fire left engine = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "# Display table with values.\n",
    "#utils.display_table(current_state, action, next_state, reward, done)\n",
    "\n",
    "# Replace the `current_state` with the state after the action is taken\n",
    "current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Deep Q-Learning\n",
    "\n",
    "In cases where both the state and action space are discrete we can estimate the action-value function iteratively by using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q_{i+1}(s,a) = R + \\gamma \\max_{a'}Q_i(s',a')\n",
    "$$\n",
    "\n",
    "This iterative method converges to the optimal action-value function $Q^*(s,a)$ as $i\\to\\infty$. This means that the agent just needs to gradually explore the state-action space and keep updating the estimate of $Q(s,a)$ until it converges to the optimal action-value function $Q^*(s,a)$. However, in cases where the state space is continuous it becomes practically impossible to explore the entire state-action space. Consequently, this also makes it practically impossible to gradually estimate $Q(s,a)$ until it converges to $Q^*(s,a)$.\n",
    "\n",
    "In the Deep $Q$-Learning, we solve this problem by using a neural network to estimate the action-value function $Q(s,a)\\approx Q^*(s,a)$. We call this neural network a $Q$-Network and it can be trained by adjusting its weights at each iteration to minimize the mean-squared error in the Bellman equation.\n",
    "\n",
    "Unfortunately, using neural networks in reinforcement learning to estimate action-value functions has proven to be highly unstable. Luckily, there's a couple of techniques that can be employed to avoid instabilities. These techniques consist of using a ***Target Network*** and ***Experience Replay***. We will explore these two techniques in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Target Network\n",
    "\n",
    "We can train the $Q$-Network by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation, where the target values are given by:\n",
    "\n",
    "$$\n",
    "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
    "$$\n",
    "\n",
    "where $w$ are the weights of the $Q$-Network. This means that we are adjusting the weights $w$ at each iteration to minimize the following error:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "Notice that this forms a problem because the $y$ target is changing on every iteration. Having a constantly moving target can lead to oscillations and instabilities. To avoid this, we can create\n",
    "a separate neural network for generating the $y$ targets. We call this separate neural network the **target $\\hat Q$-Network** and it will have the same architecture as the original $Q$-Network. By using the target $\\hat Q$-Network, the above error becomes:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "where $w^-$ and $w$ are the weights of the target $\\hat Q$-Network and $Q$-Network, respectively.\n",
    "\n",
    "In practice, we will use the following algorithm: every $C$ time steps we will use the $\\hat Q$-Network to generate the $y$ targets and update the weights of the target $\\hat Q$-Network using the weights of the $Q$-Network. We will update the weights $w^-$ of the the target $\\hat Q$-Network using a **soft update**. This means that we will update the weights $w^-$ using the following rule:\n",
    " \n",
    "$$\n",
    "w^-\\leftarrow \\tau w + (1 - \\tau) w^-\n",
    "$$\n",
    "\n",
    "where $\\tau\\ll 1$. By using the soft update, we are ensuring that the target values, $y$, change slowly, which greatly improves the stability of our learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "In this exercise you will create the $Q$ and target $\\hat Q$ networks and set the optimizer. Remember that the Deep $Q$-Network (DQN) is a neural network that approximates the action-value function $Q(s,a)\\approx Q^*(s,a)$. It does this by learning how to map states to $Q$ values.\n",
    "\n",
    "To solve the Lunar Lander environment, we are going to employ a DQN with the following architecture:\n",
    "\n",
    "* An `Input` layer that takes `state_size` as input.\n",
    "\n",
    "* A `Dense` layer with `64` units and a `relu` activation function.\n",
    "\n",
    "* A `Dense` layer with `64` units and a `relu` activation function.\n",
    "\n",
    "* A `Dense` layer with `num_actions` units and a `linear` activation function. This will be the output layer of our network.\n",
    "\n",
    "\n",
    "In the cell below you should create the $Q$-Network and the target $\\hat Q$-Network using the model architecture described above. Remember that both the $Q$-Network and the target $\\hat Q$-Network have the same architecture.\n",
    "\n",
    "Lastly, you should set `Adam` as the optimizer with a learning rate equal to `ALPHA`. Recall that `ALPHA` was defined in the [Hyperparameters](#2) section. We should note that for this exercise you should use the already imported packages:\n",
    "```python\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = Sequential([\n",
    "    Input(shape = state_size),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(num_actions, activation = 'linear')\n",
    "])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape = state_size),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(num_actions, activation = 'linear')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate = ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Experience Replay\n",
    "\n",
    "When an agent interacts with the environment, the states, actions, and rewards the agent experiences are sequential by nature. If the agent tries to learn from these consecutive experiences it can run into problems due to the strong correlations between them. To avoid this, we employ a technique known as **Experience Replay** to generate uncorrelated experiences for training our agent. Experience replay consists of storing the agent's experiences (i.e the states, actions, and rewards the agent receives) in a memory buffer and then sampling a random mini-batch of experiences from the buffer to do the learning. The experience tuples $(S_t, A_t, R_t, S_{t+1})$ will be added to the memory buffer at each time step as the agent interacts with the environment.\n",
    "\n",
    "For convenience, we will store the experiences as named tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Deep Q-Learning Algorithm with Experience Replay\n",
    "\n",
    "Now that we know all the techniques that we are going to use, we can put them together to arrive at the Deep Q-Learning Algorithm With Experience Replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "In this exercise you will implement line ***12*** of the algorithm outlined in *Fig 3* above and you will also compute the loss between the $y$ targets and the $Q(s,a)$ values. In the cell below, complete the `compute_loss` function by setting the $y$ targets equal to:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_j =\n",
    "    \\begin{cases}\n",
    "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
    "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here are a couple of things to note:\n",
    "\n",
    "* The `compute_loss` function takes in a mini-batch of experience tuples. This mini-batch of experience tuples is unpacked to extract the `states`, `actions`, `rewards`, `next_states`, and `done_vals`. You should keep in mind that these variables are *TensorFlow Tensors* whose size will depend on the mini-batch size. For example, if the mini-batch size is `64` then both `rewards` and `done_vals` will be TensorFlow Tensors with `64` elements.\n",
    "\n",
    "\n",
    "* Using `if/else` statements to set the $y$ targets will not work when the variables are tensors with many elements. However, notice that you can use the `done_vals` to implement the above in a single line of code. To do this, recall that the `done` variable is a Boolean variable that takes the value `True` when an episode terminates at step $j+1$ and it is `False` otherwise. Taking into account that a Boolean value of `True` has the numerical value of `1` and a Boolean value of `False` has the numerical value of `0`, you can use the factor `(1 - done_vals)` to implement the above in a single line of code. Here's a hint: notice that `(1 - done_vals)` has a value of `0` when `done_vals` is `True` and a value of `1` when `done_vals` is `False`. \n",
    "\n",
    "Lastly, compute the loss by calculating the Mean-Squared Error (`MSE`) between the `y_targets` and the `q_values`. To calculate the mean-squared error you should use the already imported package `MSE`:\n",
    "```python\n",
    "from tensorflow.keras.losses import MSE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    # Compute max Q^(s, a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis = -1)\n",
    "\n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    y_targets = rewards + ((1-done_vals) * gamma * max_qsa)\n",
    "\n",
    "    # Get q_values and reshape to match target\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]), tf.cast(actions, tf.int32)], axis=1))\n",
    "\n",
    "    # Compute loss\n",
    "    loss = MSE(y_targets, q_values)\n",
    "\n",
    "    return loss\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
