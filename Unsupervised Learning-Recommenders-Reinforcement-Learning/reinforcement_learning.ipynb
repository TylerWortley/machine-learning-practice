{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the goal is to find a function that maps the **state (s)** to an **action (a)**\n",
    "- uses a reward function to train model **R(s)**\n",
    "\n",
    "Applications\n",
    "- Controlling robots\n",
    "- factory optimization\n",
    "- financial (stock) trading\n",
    "- playing games (including video games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Return in Reinforcement Learning\n",
    "-  **discount factor (γ)**: modifies reward credited to each step, discounting rewards further in the future (often a number close to 1)\n",
    "- the **return** in reinforcement learning is the **sum of the rewards the system gets** but **weighted by the discount factor** -> rewards in the future are weighted by the discount factor raised to a higher power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Decisions: Policies in Reinforcement Learning\n",
    "- a policy function, **π(s) = a**, tells you what **action (a)** to take in a given **state (s)**\n",
    "\n",
    "- **goal**: fund a policy that tells you what action to take in every state (s) so as to maximize the return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "- model for sequential decision making when outcomes are uncertain and partly controllable\n",
    "- \"Markov\" means that the future only depends on the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-Action Value Function (Q Function, Q*. Optimal Q Function)\n",
    "- a function typicall denoted by **Q(s, a)**\n",
    "    - gives a number equal to the return if you start in a **state (s)**, take the **action (a)** once, and behave optimally after that\n",
    "    - tells us how good it is to take action a in state s\n",
    "    - the best possible return from **state (s)** is **max Q(s, a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "- helps compute the state-action value function (**Q(s, a)**)\n",
    "- terms\n",
    "    - **s: current state**\n",
    "    - **a: current action**\n",
    "    - **s': state you get to after taking action a**\n",
    "    - **a': action that you take in state s'**\n",
    "    - **R(s) = rewards of current state**\n",
    "- Equation: **Q(s, a) = R(s) + γ(max Q(s', a'))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Environments\n",
    "- sometimes, when you take an action, the outcome is not always completely reliable (i.e slippery floor causes robot to move in wrong direction, device thrown off balance, wind, etc)\n",
    "- there isn't one sequence of rewards that you are guarenteed to see\n",
    "- now, we are trying to maximize the **average or expected return**\n",
    "- **Q(s, a) = R(s) + γ* E(max Q(s', a'))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the State-Value Function\n",
    "- key idea: train a neural network to compute / approximate the state action value function ((Q(s, a))) that will in turn let us pick good actions\n",
    "- get Q(s, a) from NN, compute all options and select action that yeilds best reward\n",
    "- for training \n",
    "    $$ x^{(1)} = (s^{(1)}, a^{(1)}) $$ \n",
    "\n",
    "    $$ y^{(1)} = R(s^{(1)}) + γmax Q(s^{'(1)}, a^{'}) $$ \n",
    "- at every step, Q will be a guess (that will get better over time)\n",
    "\n",
    "### Full Algorithm\n",
    "1. Initialize neural network parameters randomly as an initial random guess for the Q function \n",
    "2. Repeat \n",
    "    - Take actions in lunar lander. Get (s, a, R(s), s')\n",
    "    - Store 10,000 most recent tuples (s, a, R(s), s') (called the replay buffer)\n",
    "    - Occasionally train NN by creating training set of 10,000 examples -> x =(s, a), y = R(s) + γ*max(Q(s',a'))\n",
    "    - Train such that $Q_{new} (s, a) ~= y$\n",
    "    - set $ Q = Q_{new}$ \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Refinement: Improved Neural Network Architecture\n",
    "- previous would have to carry out inference in the neural network separetly for as many times as there are actions, to pick the action that gives the largest Q value\n",
    "- instead, it is more efficient to train a single neural network to **output all action values simultaneously**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Refinement: ϵ-greedy policy\n",
    "- this is used to pick the actions while still learning (first bullet in full algorithm)\n",
    "- with probability 0.95 (**95%** of the time), pick the action a that **maximizes Q(s, a)** -> Greedy/Explotation Step\n",
    "- with probaility 0.05 (other **5%** of the time), pick action a **randomly** -> Exploration Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Refinement: Mini-Batch and Soft Updates\n",
    "- **Mini Batches**\n",
    "    - every step of gradient descent requires computing the error across every single example which can be quite slow\n",
    "    - instead, pick a subset (m') and instead of using all examples, use m' examples to speed up the algorithm\n",
    "    - good for both supervised and reinforcement learning\n",
    "- **Soft Updates**\n",
    "    - whenever we train a new neural network (Q_new), we will only update with a little portion of the new value \n",
    "        - $ W = 0.01*W_{new} + 0.99*W $\n",
    "        - $ B = 0.01*B_{new} + 0.99*B $\n",
    "    - increases convergence reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning - Lunar Lander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `numpy` is a package for scientific computing in python.\n",
    "- `deque` will be our data structure for our memory buffer.\n",
    "- `namedtuple` will be used to store the experience tuples.\n",
    "- The `gym` toolkit is a collection of environments that can be used to test reinforcement learning algorithms. We should note that in this notebook we are using `gym` version `0.24.0`.\n",
    "- `PIL.Image` and `pyvirtualdisplay` are needed to render the Lunar Lander environment.\n",
    "- We will use several modules from the `tensorflow.keras` framework for building deep learning models.\n",
    "- `utils` is a module that contains helper functions for this assignment. You do not need to modify the code in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Action Space\n",
    "\n",
    "The agent has four discrete actions available:\n",
    "\n",
    "* Do nothing.\n",
    "* Fire right engine.\n",
    "* Fire main engine.\n",
    "* Fire left engine.\n",
    "\n",
    "Each action has a corresponding numerical value:\n",
    "\n",
    "```python\n",
    "Do nothing = 0\n",
    "Fire right engine = 1\n",
    "Fire main engine = 2\n",
    "Fire left engine = 3\n",
    "```\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Observation Space\n",
    "\n",
    "The agent's observation space consists of a state vector with 8 variables:\n",
    "\n",
    "* Its $(x,y)$ coordinates. The landing pad is always at coordinates $(0,0)$.\n",
    "* Its linear velocities $(\\dot x,\\dot y)$.\n",
    "* Its angle $\\theta$.\n",
    "* Its angular velocity $\\dot \\theta$.\n",
    "* Two booleans, $l$ and $r$, that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Rewards\n",
    "\n",
    "After every step, a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "- is increased/decreased the closer/further the lander is to the landing pad.\n",
    "- is increased/decreased the slower/faster the lander is moving.\n",
    "- is decreased the more the lander is tilted (angle not horizontal).\n",
    "- is increased by 10 points for each leg that is in contact with the ground.\n",
    "- is decreased by 0.03 points each frame a side engine is firing.\n",
    "- is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receives an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Episode Termination\n",
    "\n",
    "An episode ends (i.e the environment enters a terminal state) if:\n",
    "\n",
    "* The lunar lander crashes (i.e if the body of the lunar lander comes in contact with the surface of the moon).\n",
    "\n",
    "* The absolute value of the lander's $x$-coordinate is greater than 1 (i.e. it goes beyond the left or right border)\n",
    "\n",
    "You can check out the [Open AI Gym documentation](https://www.gymlibrary.dev/environments/box2d/lunar_lander/) for a full description of the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v3', render_mode  ='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAGQAlgDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK1PD2lnV9ZhtiMxg75B/sjt1B5OBx0zntWXXeeDLnT9L0W7vnmRphlpFzygHCgjPvwQP4sV0YWnGpVSltuwH+PdWEEEWkwN8xG+U98en413Fv/AKo/77/+hGvEL+8l1C+mu5jl5WLH29q34/Herxxhdtux7sVYEnueGA/Ku+hmEI1Zzns9vkO56rRXln/Cfav/AM87b8n/APiqP+E+1f8A55235P8A/FV2f2rR8wuep1w3je+fTdc0m7Rd3lhyV/vLkAjocZBIz2rF/wCE+1b/AJ5W35P/APFVlazrt1rjQtdJEpiBClN3Ocdck+lc2LzCnUpcsN9AbO+1+yh8SeG/tFq6zSQjzIpF/iH/AOrt2IPpXlpGDg12XgzxHDp8clney7IwS0Rbpj+Jck4HqPq3ciuc1mazuNVnmsVdYHYkBhjnPb2+tceLnCrGNVPV7oRQooorhAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAor6TvfAnhe/VRNolqm3keQph/8AQMZrm774OaHP5jWd5eWrt90ErIi/gQCf++q+fpcSYSfxpx+V/wAv8jV0ZHiFFel3/wAGdUg+az1SznjAyxnDREfluH6155f2UunX01pM0TSRNtYxSLIp+jKSP8Oh5r1sNjsPif4MrkOLjuV6KKK6iQooooAKKKKACiiigAooooAKKK+k73wJ4Xv1UTaJapt5HkKYf/QMZrzcwzOngXD2ib5r7eVv8y4Qctj5sor2+++Dmhz+Y1neXlq7fdBKyIv4EAn/AL6rm7/4M6pB81nqlnPGBljOGiI/LcP1rKlnuBqfbs/NP/hhulJHmlFWL+yl06+mtJmiaSJtrGKRZFP0ZSR/h0PNV69ZNSV1sZhRRRTAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA9f8AFfxW1DSPEGo6TY6dbA2N3LbGadmfzNjFchRtxnGeprjL74k+Kr/zFOptBG/8FvGqbfowG79ap+O/+Sh+Jf8AsK3X/o1q5+uClleDpfDTXz1/O5bnJ9Szeaje6i4e+vLi5deA08rOR+ZqtRRXcoqKsiAooopgFFFFABRRRQAUUUUAFFFFABXr/iv4rahpHiDUdJsdOtgbG7ltjNOzP5mxiuQo24zjPU15BXQeO/8AkofiX/sK3X/o1q5cTgqGJcXWje23zKUnHYuX3xJ8VX/mKdTaCN/4LeNU2/RgN361zl5qN7qLh768uLl14DTys5H5mq1FXSw1Gj/Dgl6ITbe4UUUVuIKKKKACiiigAooooAKKKKACiiigAoopVVnYKoJYnAAHJNACUV2OifCzxpr+1rTQrmKFv+W10BCuPUb8Ej6A16Ron7Nly+2TXtdjiHeGyjLn/vtsY/75NAHg1X9L0TVdbn8nS9Nu72TutvCz4+uBxX1lonwZ8D6JtYaQL6Zf+Wl+3m5/4D9z/wAdrube3gtIFhtoY4YlGFSNQqj6AUAfLGifALxlqm175LXS4jyftEu58eypn8iRXYXX7NUI0ZhaeIXfVQcq00AWBuD8pAJZecfNk4APynPHvlFAHwz4k8K614S1NrDWbGS3kydjkZjlAxyjdGHI6dM4ODxWNX3jrGjabr+mS6dqtnFd2kow0cg/UHqCOxGCO1fP3jv9n+9sWkv/AAi7XltyzWMrjzk/3D0cdeDhuB94mgDw+inzQy288kE8bxSxsUeN1KsrA4IIPQg0ygAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOg8d/8AJQ/Ev/YVuv8A0a1c/XQeO/8AkofiX/sK3X/o1q5+gAooooAKKKKACiiigAooooAKKKKACiiigAroPHf/ACUPxL/2Fbr/ANGtXP10Hjv/AJKH4l/7Ct1/6NagDn6KKKACiiun8NfD3xR4tZW0rSZmtyf+PqUeXCP+BHg/QZNAHMVPZ2V3qN0lrZWs1zcOcJFBGXdvoBya+hfDH7OVhb7J/E2pvdyDk21nlI/oXI3MPoFr1/RPDmi+HLX7Po+mW1lHj5vKjAZv95urH3JNAHzJo/wD8a6raC4mSx03P3Y7yYhyPXCK2PocGtRf2b/FX8Wq6MPpJKf/AGSvpuigD5oX9m7xH/FrOlD6eYf/AGWpF/Zs13+LXdOH0Rz/AEr6TooA+cF/Zq1b+LxDZD6QualX9mnUP4vEtqPpasf/AGavoqigD55X9me6/i8Uwj6WRP8A7PUi/szP/F4sUfTT8/8AtSvoKigDx/RP2d/C9htfVry81SQdVz5EZ/Bfm/8AHq9H0Xwn4f8ADqgaRo9naMBjzI4hvP1b7x/E1s0UAFFFFABRRRQAUUUUAFFFFAHIeNfht4e8c25/tC28m+AxHfW4CyrjoCcfMvsffGDzXzT44+FPiLwRI88sJvdLB+W+t1+UD/bXqh+vHPBNfYtI6LIjI6hlYYKkZBHpQB8A0V9OeO/gLpWteZfeGmj0u+OWNtj/AEeQ+wH+r/DI9h1r5317w7q/hjUWsNZsJrS4HQOOHHqrDhh7gmgDLooooAKKKKACiiigAooooAKKKKACiiigAoop8UMs8qRQxvJI5wqIpJY+gA60AMor07wv8CvFmv7Jr+JdGtG5LXYPmke0Y5z7Ntr2vwv8E/CHhzZNPaHVbxefOvcMoPtH9388n3oA+a/DPgDxP4udTpGlTSQE4NzINkI/4GeD9Bk+1S+Lfh14m8FyE6rp7G1zhbyD54W9Pm/hz6MAfavtRVVEVEUKqjAAGABSSRpLG0ciK6OCrKwyCD1BFAHwFRX1P4w+A3hzXvMutGP9jXx52xLugc+8f8Pp8pAHoa8C8W/DrxN4LkJ1XT2NrnC3kHzwt6fN/Dn0YA+1AHK0UUUAFFFFABRRRQAUUUUAFFFFABRRXV+G/ht4s8VbH03R5vszf8vM48qLHqGb73/Ac0AVPHf/ACUPxL/2Fbr/ANGtXP17hrvwE8Ya34h1PVvtmhw/bruW58r7TM2ze5bbnyhnGcZwKz/+GcfGH/QS0P8A7/zf/GqAPH6K9g/4Zx8Yf9BLQ/8Av/N/8ao/4Zx8Yf8AQS0P/v8Azf8AxqgDx+ivYP8AhnHxh/0EtD/7/wA3/wAao/4Zx8Yf9BLQ/wDv/N/8aoA8for2D/hnHxh/0EtD/wC/83/xqj/hnHxh/wBBLQ/+/wDN/wDGqAPH6K9g/wCGcfGH/QS0P/v/ADf/ABqj/hnHxh/0EtD/AO/83/xqgDx+ivYP+GcfGH/QS0P/AL/zf/GqP+GcfGH/AEEtD/7/AM3/AMaoA8for2D/AIZx8Yf9BLQ/+/8AN/8AGqP+GcfGH/QS0P8A7/zf/GqAPH66Dx3/AMlD8S/9hW6/9GtXoH/DOPjD/oJaH/3/AJv/AI1XeW3wFsNS8Q6hrnia9eWS9u5bo2Nm5Eab3LbTIQGcDOMgJQB81Wdld6jdJa2VrNc3DnCRQRl3b6AcmvUvDHwA8T6xsm1eSHR7Y8kSfvJiPZAcD8SD7V9JaJ4c0Xw5a/Z9H0y2so8fN5UYDN/vN1Y+5JrUoA8/8MfBrwd4a2S/YP7Ru158+/xJg+yY2j24yPWu/ACqFUAADAA7UtFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFZmu+HtJ8Tac1hrFjDd27dFccqfVWHKn3BBrTooA+ZPHfwE1TRvNv/DLSanYjLG1IzcRj2A/1g+mD7HrRX03RQB826h+zZrcWf7O12wuQOn2iN4Sfy31ymofBHx7YZK6Qt0g/itrhG/QkN+lfXtFAHwxqHhPxFpOTqGhalbKP4pbV1X88YNY9ff8AWbqHh3RNWz/aOj2F4T1NxbI5/MigD4Sor7C1D4MeAtQyW0Jbdz/FbTPHj8Adv6Vyuofs3+HZsnT9X1K1Y9pdkqj8MKf1oA+Z6K9t1D9mzW4s/wBna7YXIHT7RG8JP5b6wofgD44k1FbaSCyigPW6a5BQfgPm/SgDy+tbQvDOt+Jrr7Po2mXN7IDhjGnyp/vMeF/Eivo3wv8As/8AhrSNk+syy6xcjna/7uEH/dByfxOD6V6rZ2Vrp9qlrZW0NtbxjCRQxhEUewHAoA8A8L/s5Ty7LjxRqYhXqbSy+ZvxkPA/AH617P4c8FeHPCcQTRtKgt3xhpsbpW+rnLfhnFb9FABRRRQAUUUUAFNkjSWNo5EV0cFWVhkEHqCKdRQB5P4w+A3hzXvMutGP9jXx52xLugc+8f8AD6fKQB6GvAvFvw68TeC5CdV09ja5wt5B88Lenzfw59GAPtX2rTZI0ljaORFdHBVlYZBB6gigD4Cor6n8YfAbw5r3mXWjH+xr487Yl3QOfeP+H0+UgD0NeBeLfh14m8FyE6rp7G1zhbyD54W9Pm/hz6MAfagDlaKKsWVhealdLa2NpPdXD/digjLsfoBzQBXor1bw58AvFesbJdTMGj27cnzj5kuPZFP6EivXfDfwL8HaFslu7eTVrkc77w5TPtGOMezbqAPmLQ/DGueJZ/J0bS7q9YHDNEnyr/vN0X8SK9Z8N/s5apdbJvEWpxWUfU29qPNk+hY/Kp+m6voy3t4bSBILaGOGFBhI41Cqo9ABwKkoA43w38K/B/hjY9npEU9yv/Lzd/vpM+ozwp/3QK7KiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApskaSxtHIiujgqysMgg9QRTqKAPPL/4JeBr/AFldRbS3hHJe1t5THC59do+79FIHtXZ6ToWlaDa/ZtJ062soe6wRhd3uSOp9zWhRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .reset() method to reset the environemtn to initial state. lander starts at the top center of the environment and we can render the first frame \n",
    "# of the environmnet with .render() method\n",
    "env.reset()\n",
    "Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of state vector and number of valid actions required to build neural netowrk later\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Interacting with the Gym Environment\n",
    "\n",
    "In the standard “agent-environment loop” formalism, an agent interacts with the environment in discrete time steps $t=0,1,2,...$. At each time step $t$, the agent uses a policy $\\pi$ to select an action $A_t$ based on its observation of the environment's state $S_t$. The agent receives a numerical reward $R_t$ and on the next time step, moves to a new state $S_{t+1}$.\n",
    "\n",
    "<a name=\"5.1\"></a>\n",
    "### 5.1 Exploring the Environment's Dynamics\n",
    "\n",
    "In Open AI's Gym environments, we use the `.step()` method to run a single time step of the environment's dynamics. In the version of `gym` that we are using the `.step()` method accepts an action and returns four values:\n",
    "\n",
    "* `observation` (**object**): an environment-specific object representing your observation of the environment. In the Lunar Lander environment this corresponds to a numpy array containing the positions and velocities of the lander as described in section [3.2 Observation Space](#3.2).\n",
    "\n",
    "\n",
    "* `reward` (**float**): amount of reward returned as a result of taking the given action. In the Lunar Lander environment this corresponds to a float of type `numpy.float64` as described in section [3.3 Rewards](#3.3).\n",
    "\n",
    "\n",
    "* `done` (**boolean**): When done is `True`, it indicates the episode has terminated and it’s time to reset the environment. \n",
    "\n",
    "\n",
    "* `info` (**dictionary**): diagnostic information useful for debugging. We won't be using this variable in this notebook but it is shown here for completeness.\n",
    "\n",
    "To begin an episode, we need to reset the environment to an initial state. We do this by using the `.reset()` method. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
