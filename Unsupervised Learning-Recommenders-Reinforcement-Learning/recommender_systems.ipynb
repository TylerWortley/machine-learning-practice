{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Notation\n",
    "- r(i,j) = 1 if user j has rater movie i (0 if they have not yet rated)\n",
    "- $y^{(i,j)}$ = rating fiven by user j on movie i (if defined)\n",
    "- $w^{(j)}$, $b^{(j)}$ = parameters for user j\n",
    "- $x^{(i)}$ = feature vector for movie i\n",
    "- $m^{(j)}$ = number of movies rated by user j\n",
    "\n",
    "For user j and movie i, predicted rating =  $w^{(j)}$ ⋅ $x^{(i)}$ + $b^{(j)}$\n",
    "\n",
    "**Cost Function** \n",
    "$$\\frac{1}{2} \\sum_{i:r(i,j) = 1} (w^{(j)} ⋅ x^{(i)} + b^{(j)} - y^{(i,j)}) + \\frac{λ}{2} \\sum_{k=1}^n (w_k^{(j)})^2 $$\n",
    "- sum cost function for all users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborative Filtering**\n",
    "$$\\frac{1}{2} \\sum_{i:r(i,j) = 1} (w^{(j)} ⋅ x^{(i)} + b^{(j)} - y^{(i,j)}) + \\frac{λ}{2} \\sum_{i=1}^{n_u} \\sum_{k=1}^n (w_k^{(j)})^2 \\frac{λ}{2} + \\sum_{i=1}^{n_m} \\sum_{k=1}^n (w_k^{(j)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cost function now a function of w, b, and x rather than just w and b\n",
    "- through gradient descent, update all values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Labels**\n",
    "- rather than predicting y through a linear funciton, use logistic function\n",
    " $$ \\sum_{(i,j):r(i,j) = 1} L(f_{(w,b,x)}, y^{(i,j)}) $$\n",
    " where \n",
    "$$L(f_{(w,b,x)}, y^{(i,j)}) = -y^{(i,j)}log(f_{(w,b,x)}(x)) - (1-y^{(i,j)})log(1-f_{(w,b,x)}(x))$$\n",
    " $$ f_{(w,b,x)}(x) = g(w^{(j)} ⋅ x^{(i)} + b^{(j)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Normalizaiton**\n",
    "- when there is a new user that hasnt provided any ratings, normaize the rows of the current rating matrix to be zero\n",
    " $$ w^{(j)} ⋅ x^{(i)} + b^{(j)} + μ_{1} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent Implementation of Collaborative Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(3.0) # tf variables are the parameters we want to optimize\n",
    "x = 1.0\n",
    "y = 1.0 # target value\n",
    "alpha = 0.01\n",
    "\n",
    "iterations = 30\n",
    "for iter in range(iterations):\n",
    "\n",
    "    # use TensorFlow's Gradient tape to record the steps used to compute tje cost J, to enable auto differentiaion\n",
    "    with tf.GradientTape() as tape:\n",
    "        fwb = w*x\n",
    "        costJ = (fwb - y)**2\n",
    "    \n",
    "    # Use the gradient tape to calculate the gradients of the cost with respect to the parameter w\n",
    "    [dJdw] = tape.gradient(costJ, [w])\n",
    "\n",
    "    # Run one step of gradient descent by updating the value of w to reduce the cost\n",
    "    w.assign_add(-alpha * dJdw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam Optimizer Implementation of Collaborative Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# instance of optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 1e-1)\n",
    "\n",
    "iterations = 200\n",
    "for iter in range(iterations):\n",
    "    # use TensorFlow's GradientTape to record operatiomns used to compute the cost\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # compute cost (forward pass is included in cost)\n",
    "        cost_value = coifCostFuncV(X, W, b, Ynorm, R, num_users, num_movies, lambda)\n",
    "\n",
    "        # use the fradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss\n",
    "        grads = tape.gradient(cost_value, [X, W, b])\n",
    "\n",
    "        # Run one step of gradient descent by updating the value of the variables to minimize the loss\n",
    "        optimizer.apply_gradients(zip(grads, [X, W, b]))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
