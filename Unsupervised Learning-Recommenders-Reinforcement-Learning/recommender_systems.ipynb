{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Function**\n",
    "\n",
    "Notation\n",
    "- r(i,j) = 1 if user j has rater movie i (0 if they have not yet rated)\n",
    "- $y^{(i,j)}$ = rating fiven by user j on movie i (if defined)\n",
    "- $w^{(j)}$, $b^{(j)}$ = parameters for user j\n",
    "- $x^{(i)}$ = feature vector for movie i\n",
    "- $m^{(j)}$ = number of movies rated by user j\n",
    "\n",
    "For user j and movie i, predicted rating =  $w^{(j)}$ ⋅ $x^{(i)}$ + $b^{(j)}$\n",
    "\n",
    "**Cost Function** \n",
    "$$\\frac{1}{2} \\sum_{i:r(i,j) = 1} (w^{(j)} ⋅ x^{(i)} + b^{(j)} - y^{(i,j)}) + \\frac{λ}{2} \\sum_{k=1}^n (w_k^{(j)})^2 $$\n",
    "- sum cost function for all users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborative Filtering**\n",
    "$$\\frac{1}{2} \\sum_{i:r(i,j) = 1} (w^{(j)} ⋅ x^{(i)} + b^{(j)} - y^{(i,j)}) + \\frac{λ}{2} \\sum_{i=1}^{n_u} \\sum_{k=1}^n (w_k^{(j)})^2 \\frac{λ}{2} + \\sum_{i=1}^{n_m} \\sum_{k=1}^n (w_k^{(j)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cost function now a function of w, b, and x rather than just w and b\n",
    "- through gradient descent, update all values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Labels**\n",
    "- rather than predicting y through a linear funciton, use logistic function\n",
    " $$ \\sum_{(i,j):r(i,j) = 1} L(f_{(w,b,x)}, y^{(i,j)}) $$\n",
    " where \n",
    "$$L(f_{(w,b,x)}, y^{(i,j)}) = -y^{(i,j)}log(f_{(w,b,x)}(x)) - (1-y^{(i,j)})log(1-f_{(w,b,x)}(x))$$\n",
    " $$ f_{(w,b,x)}(x) = g(w^{(j)} ⋅ x^{(i)} + b^{(j)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Normalizaiton**\n",
    "- when there is a new user that hasnt provided any ratings, normaize the rows of the current rating matrix to be zero\n",
    " $$ w^{(j)} ⋅ x^{(i)} + b^{(j)} + μ_{1} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent Implementation of Collaborative Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(3.0) # tf variables are the parameters we want to optimize\n",
    "x = 1.0\n",
    "y = 1.0 # target value\n",
    "alpha = 0.01\n",
    "\n",
    "iterations = 30\n",
    "for iter in range(iterations):\n",
    "\n",
    "    # use TensorFlow's Gradient tape to record the steps used to compute tje cost J, to enable auto differentiaion\n",
    "    with tf.GradientTape() as tape:\n",
    "        fwb = w*x\n",
    "        costJ = (fwb - y)**2\n",
    "    \n",
    "    # Use the gradient tape to calculate the gradients of the cost with respect to the parameter w\n",
    "    [dJdw] = tape.gradient(costJ, [w])\n",
    "\n",
    "    # Run one step of gradient descent by updating the value of w to reduce the cost\n",
    "    w.assign_add(-alpha * dJdw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam Optimizer Implementation of Collaborative Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# instance of optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 1e-1)\n",
    "\n",
    "iterations = 200\n",
    "for iter in range(iterations):\n",
    "    # use TensorFlow's GradientTape to record operatiomns used to compute the cost\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # compute cost (forward pass is included in cost)\n",
    "        cost_value = coifCostFuncV(X, W, b, Ynorm, R, num_users, num_movies, lambda)\n",
    "\n",
    "        # use the fradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss\n",
    "        grads = tape.gradient(cost_value, [X, W, b])\n",
    "\n",
    "        # Run one step of gradient descent by updating the value of the variables to minimize the loss\n",
    "        optimizer.apply_gradients(zip(grads, [X, W, b]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| General Notation       | Description                                                             | Python (if any) |\n",
    "|:-----------------------|:------------------------------------------------------------------------|:----------------|\n",
    "| $r(i,j)$               | scalar; = 1 if user j rated movie i, = 0 otherwise                      |                 |\n",
    "| $y(i,j)$               | scalar; rating given by user j on movie i (if $r(i,j) = 1$ is defined)  |                 |\n",
    "| $\\mathbf{w}^{(j)}$     | vector; parameters for user j                                           |                 |\n",
    "| $b^{(j)}$              | scalar; parameter for user j                                            |                 |\n",
    "| $\\mathbf{x}^{(i)}$     | vector; feature ratings for movie i                                     |                 |\n",
    "| $n_u$                 | number of users                                                         | `num_users`     |\n",
    "| $n_m$                 | number of movies                                                        | `num_movies`    |\n",
    "| $n$                   | number of features                                                      | `num_features`  |\n",
    "| $\\mathbf{X}$           | matrix of vectors $\\mathbf{x}^{(i)}$                                    | `X`             |\n",
    "| $\\mathbf{W}$           | matrix of vectors $\\mathbf{w}^{(j)}$                                    | `W`             |\n",
    "| $\\mathbf{b}$           | vector of bias parameters $b^{(j)}$                                     | `b`             |\n",
    "| $\\mathbf{R}$           | matrix of elements $r(i,j)$                                             | `R`             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal of Collaborative Filtering** - generate 2 vectors of the same size\n",
    "- For **each user**: a parameter vector that embodies the movie tastes of a user\n",
    "- For **each movie**: a feature vector that embodies the description of the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coif_cost_func(X, W, b, Y, R, lambda_):\n",
    "        x\n",
    "        \"\"\"\n",
    "        Returns the cost for the content-based filtering\n",
    "        Args:\n",
    "        X (ndarray (num_movies,num_features)): matrix of item features\n",
    "        W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "        b (ndarray (1, num_users)            : vector of user parameters\n",
    "        Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "        R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "        lambda_ (float): regularization parameter\n",
    "        Returns:\n",
    "        J (float) : Cost\n",
    "        \"\"\"\n",
    "        nm, nu = Y.shape\n",
    "        J = 0\n",
    "\n",
    "        for j in range(nu):\n",
    "            w = W[j, :]\n",
    "            b_j = b[0, j]\n",
    "            for i in range(nm):\n",
    "                x = X[i, :]\n",
    "                y = Y[i, j]\n",
    "                r = R[i, j]\n",
    "                J += 1/2 * (r * (np.dot(w, x) + b_j - y)**2)\n",
    "            \n",
    "        # adding regularization\n",
    "        J += lambda_/2 * (np.sum(np.square(W)) + np.sum(np.square(X)))\n",
    "\n",
    "        return J\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorized Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func_v(X, W, b, Y, R, lambda_):\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Useful Values\n",
    "num_movies, num_users = Y.shape\n",
    "num_features = 100\n",
    "\n",
    "# Set Initial Parameters (W, X), use tf.Variable to track these variables\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "W = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name='W')\n",
    "X = tf.Variable(tf.random.normal((num_movies, num_features),dtype=tf.float64),  name='X')\n",
    "b = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-1)\n",
    "\n",
    "iterations = 200\n",
    "lambda_ = 1\n",
    "\n",
    "for iter in range(iterations):\n",
    "    # Use TensorFlow’s GradientTape\n",
    "    # to record the operations used to compute the cost \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Compute the cost (forward pass included in cost)\n",
    "        cost_value = cofi_cost_func_v(X, W, b, Ynorm, R, lambda_)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss\n",
    "    grads = tape.gradient( cost_value, [X,W,b] )\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients( zip(grads, [X,W,b]) )\n",
    "\n",
    "    # Log periodically.\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Training loss at iteration {iter}: {cost_value:0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using trained weights and biases\n",
    "p = np.matmul(X.numpy(), np.transpose(W.numpy())) + b.numpy()\n",
    "\n",
    "#restore the mean\n",
    "pm = p + Ymean\n",
    "\n",
    "my_predictions = pm[:,0]\n",
    "\n",
    "# sort predictions\n",
    "ix = tf.argsort(my_predictions, direction='DESCENDING')\n",
    "\n",
    "for i in range(17):\n",
    "    j = ix[i]\n",
    "    if j not in my_rated:\n",
    "        print(f'Predicting rating {my_predictions[j]:0.2f} for movie {movieList[j]}')\n",
    "\n",
    "print('\\n\\nOriginal vs Predicted ratings:\\n')\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0:\n",
    "        print(f'Original {my_ratings[i]}, Predicted {my_predictions[i]:0.2f} for {movieList[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the difference?**\n",
    "- **Collaborative Filtering**: recommends items based on ratings of users who gave similar ratings as you\n",
    "- **Content-Based Fitlering**: recommends items based on features of user and item to find a good match -> requires features of users and items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "- User Features $x_u^{(j)}$\n",
    "    - Age\n",
    "    - Gender\n",
    "    - Country\n",
    "    - Movies watched\n",
    "\n",
    "- Movie Filters $x_m^{{i}}$\n",
    "    - Year\n",
    "    - Genre(s)\n",
    "    - Reviews\n",
    "    - Average ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using a few layers, will output a vector ($v_u$) that describes the user\n",
    "- inputs can be different size, outputs same size\n",
    "- combine together computing $v_u$ ⋅ $v_m$\n",
    "- training with a single cost function using graiend descent or other optimization\n",
    "- **cost function**: J = $ \\sum_{(i,j):r(i,j) = 1} (v_u^{(j)} ⋅ v_m^{(i)}  - y^{(i,j)})^2 $\n",
    "\n",
    "**User Network**\n",
    "- takes as input: features of user\n",
    "- output: $v_u$\n",
    "\n",
    "**Movie Network**\n",
    "- takes as input: features of the movie\n",
    "- output: $v_m$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommending from a Large Catalogue\n",
    "- many large scale recommender systems are implemented in 2 steps\n",
    "1. retrevial step\n",
    "- generate large list of plausable item candidates\n",
    "- combine and retrieve items into list, removing duplicates and items already watched/purchases\n",
    "2. ranking step\n",
    "- take retrieved list and rank them using learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_NN = tf.keras.models.Sequential(\n",
    "    [tf.keras.Dense(256, activation = 'relu'),\n",
    "     tf.keras.Dense(128, activation = 'relu'),\n",
    "     tf.keras.Dense(32)\n",
    "     ])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential(\n",
    "    [tf.keras.Dense(256, activation = 'relu'),\n",
    "     tf.keras.Dense(128, activation = 'relu'),\n",
    "     tf.keras.Dense(32)\n",
    "     ])\n",
    "\n",
    "# create user input and point back to base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.linalg.12_normalize(vu, axis = 1)\n",
    "\n",
    "# create item input and point back to base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.linalg.12_normalize(vm, axis = 1)\n",
    "\n",
    "# measure similarity of two vector outputs\n",
    "output = tf.keras.layers.Dot(axis=1)([vu, vm])\n",
    "\n",
    "# specify inputs and outputs of the model\n",
    "model = Model([input_user, input_item], output)\n",
    "\n",
    "# specify cost function\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
